const createCsvWriter = require("csv-writer").createObjectCsvWriter;

const scraperObject = {
  url: "https://dailyinqilab.com/health?page=1",
  async scraper(browser) {
    let page = await browser.newPage();
    let countNumber = 0;
    console.log(`Navigating to ${this.url}...`);
    // Navigate to the selected page
    await page.goto(this.url);
    // Wait for the required DOM to be rendered
    await page.waitForSelector("header");

    let data = [];
    let urls = []; // Store the URLs permanently

    // Function to scrape data from a page
    const scrapePage = async (link) => {
      let dataObj = {};
      let newPage = await browser.newPage();
      await newPage.goto(link, { timeout: 60000 });
      await newPage.waitForSelector(
        ".main-contents > .container > .row:last-child > .col-md-9 > h2"
      );
      console.log(countNumber++);

      // Scrape all h3 elements
      dataObj["blogTitles"] = await newPage.$eval(
        ".main-contents > .container > .row:last-child > .col-md-9 > h2",
        (element) => element.textContent
      );

      // Scrape all p elements
      dataObj["blogContent"] = await newPage.$$eval(
        ".main-contents > .container > .row:last-child > .col-md-9 > .new-details p",
        (elements) => elements.map((el) => el.textContent)
      );

      await newPage.close();
      return dataObj;
    };

    // Function to load more blogs by clicking the ".page-link[rel='next']" button
    const loadMoreBlogs = async () => {
      let hasMoreButton = true;
      let count = 0;
      while (hasMoreButton) {
        // Get the links to all the required blogs and store them before clicking "Next"
        const pageUrls = await page.evaluate(() => {
          const links = [];
          const linkElements = document.querySelectorAll(
            ".main-contents > .container .mt-5 a"
          );
          linkElements.forEach((el) => {
            const href = el.href;
            if (href.startsWith("https://dailyinqilab.com/")) {
              links.push(href);
            }
          });
          return links;
        });

        urls = urls.concat(pageUrls); // Concatenate the new URLs with the existing ones

        const nextPageLink = await page.$('a[rel="next"]');
        if (nextPageLink) {
          try {
            await nextPageLink.click();
            await page.waitForSelector(".main-contents > .container .mt-5 a");
            count++;
          } catch (error) {
            console.error("Error clicking next page link:", error);
            hasMoreButton = false;
          }
        } else {
          hasMoreButton = false;
        }
      }
    };

    // Load more blogs and scrape data
    await loadMoreBlogs();

    // Scrape data from each blog page and push to the data array
    for (let link of urls) {
      let currentPageData = await scrapePage(link);
      data.push(currentPageData);
    }

    // Define CSV writer and write the data to a CSV file
    const csvWriter = createCsvWriter({
      path: "scraped_data.csv",
      header: [
        { id: "blogTitles", title: "Blog Titles" },
        { id: "blogContent", title: "Blog Content" },
      ],
    });

    csvWriter
      .writeRecords(data)
      .then(() => console.log("CSV file written successfully"))
      .catch((error) => console.error("Error writing CSV file:", error));
  },
};

module.exports = scraperObject;
